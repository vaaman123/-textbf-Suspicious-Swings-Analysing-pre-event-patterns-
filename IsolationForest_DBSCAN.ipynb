{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "679defad-9d7b-4714-88f4-2cf421877852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6daea3c6-ed1f-460c-80d5-351062702ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasheet = \"TATACONSUM_data.xlsx\"\n",
    "df = pd.read_excel(datasheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0537e176-2697-4581-bb9f-1878a5d8d78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processor(df):\n",
    "    \"\"\"\n",
    "    Processes the input DataFrame 'df' containing stock data, \n",
    "    reads NIFTY data, calculates various metrics, \n",
    "    and returns a DataFrame 'proc_df' with expected return from \n",
    "    a rolling linear regression model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Create a copy for calculations that won't clutter the original\n",
    "    df = df.copy()\n",
    "\n",
    "    #event_date = pd.to_datetime(event_date)\n",
    "    \n",
    "    # 2. Basic columns for the output DataFrame\n",
    "    proc_df = pd.DataFrame()\n",
    "    proc_df['Date'] = df['Date']\n",
    "    proc_df['Stock_Price'] = df['Close']\n",
    "    \n",
    "    # 3. Stock daily returns\n",
    "    df['Stock_Daily_Returns'] = df['Close'].pct_change()\n",
    "    proc_df['Stock_Daily_Returns'] = df['Stock_Daily_Returns']\n",
    "    \n",
    "    # 4. Volume metrics\n",
    "    proc_df['Volume'] = df['Volume']\n",
    "    proc_df['Volume_change'] = df['Volume'].diff()\n",
    "    window = 10\n",
    "    df['Volume_mean'] = df['Volume'].rolling(window).mean()\n",
    "    df['Volume_std'] = df['Volume'].rolling(window).std()\n",
    "    proc_df['Volume_pct_change'] = df['Volume'].pct_change() \n",
    "    proc_df['Volume_zscore'] = (df['Volume'] - df['Volume_mean']) / df['Volume_std']\n",
    "    \n",
    "    # 5. Log returns & rolling volatility (std dev)\n",
    "    df['Log_Returns'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "    proc_df['Volatility'] = df['Log_Returns'].rolling(window).std()\n",
    "    \n",
    "    # 6. Read NIFTY data (assumes same date alignment)\n",
    "    nifty_df = pd.read_excel(\"NIFTY_data.xlsx\")\n",
    "    nifty_df['Nifty_Daily_Returns'] = nifty_df['Close'].pct_change()\n",
    "    \n",
    "    # Merge or align by Date if needed. If same length + same ordering, you can do direct assignment.\n",
    "    # Example: If they have the same index length and order:\n",
    "    df['Nifty_Daily_Returns'] = nifty_df['Nifty_Daily_Returns']\n",
    "    #    or if you must merge by Date:\n",
    "    \n",
    "    # 7. Drop NaN rows from the merges/changes\n",
    "    df.dropna(subset=['Stock_Daily_Returns', 'Nifty_Daily_Returns'], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # 8. Prepare for rolling regression\n",
    "    # We'll create a new column in proc_df for Expected_Return\n",
    "    proc_df['Expected_Return'] = np.nan\n",
    "    \n",
    "    # Set rolling window size for the regression\n",
    "    n = 10\n",
    "    \n",
    "    # 9. Rolling regression\n",
    "    for i in range(n, len(df)):\n",
    "        # Past n rows are [i-n, i-1], inclusive\n",
    "        train_df = df.iloc[i-n : i]\n",
    "        \n",
    "        # X_train must be 2D\n",
    "        X_train = train_df[['Nifty_Daily_Returns']].values\n",
    "        y_train = train_df['Stock_Daily_Returns'].values\n",
    "        \n",
    "        # We'll predict for row i (the \"today\" row),\n",
    "        # using today's Nifty return to forecast today's Stock return\n",
    "        # If you want next-day forecast, shift i accordingly\n",
    "        X_pred = df.loc[i, ['Nifty_Daily_Returns']].values.reshape(1, -1)\n",
    "        \n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        predicted_return = model.predict(X_pred)[0]\n",
    "        \n",
    "        # Put the result in proc_df for the matching date\n",
    "        # We'll find the matching date in df\n",
    "        row_date = df.loc[i, 'Date']\n",
    "        \n",
    "        # We need the index in proc_df that matches 'row_date'\n",
    "        proc_index = proc_df[proc_df['Date'] == row_date].index\n",
    "        if not proc_index.empty:\n",
    "            proc_df.at[proc_index[0], 'Expected_Return'] = predicted_return\n",
    "        prev_close = df.loc[i-1, 'Close']\n",
    "        expected_price = prev_close * (1 + predicted_return)\n",
    "        proc_df.at[i, 'Expected_Price'] = expected_price\n",
    "\n",
    "    '''proc_df['Binary_Date_Flag'] = np.nan\n",
    "    for i in range (len(proc_df)):\n",
    "        date = proc_df.at[i, 'Date']\n",
    "        if((date < event_date) and (event_date - date).days <= 30):\n",
    "            proc_df.at[i, 'Binary_Date_Flag'] = 1\n",
    "        else:\n",
    "            proc_df.at[i, 'Binary_Date_Flag'] = 0'''\n",
    "            \n",
    "        \n",
    "    proc_df['Market_Price'] = nifty_df['Close']   \n",
    "    proc_df.dropna(inplace = True)\n",
    "        \n",
    "    \n",
    "    return proc_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1d78e9f5-57f4-4a83-b41c-eb6af09a9575",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor(df).to_excel(\"processed_data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "55089a8f-c846-4f2d-be07-3b74f6c70f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = data_processor(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7ae26a5a-0e29-4c68-92c4-3ed663120e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Stock_Price', 'Stock_Daily_Returns', 'Volume', 'Volume_change',\n",
       "       'Volume_pct_change', 'Volume_zscore', 'Volatility', 'Expected_Return',\n",
       "       'Expected_Price', 'Market_Price'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d5726c28-7803-40be-8196-cf29e35c5f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_insider_trading(proc_df, contamination=0.03, random_state=42):\n",
    "    \"\"\"\n",
    "    Applies an Isolation Forest model to detect potential insider trading anomalies \n",
    "    in a preprocessed DataFrame `proc_df`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    proc_df : pd.DataFrame\n",
    "        The DataFrame returned by data_processor(...), containing numeric columns \n",
    "        for features such as 'Stock_Daily_Returns', 'Volume_zscore', etc.\n",
    "    feature_cols : list of str, optional\n",
    "        The list of column names in proc_df to use as features for the Isolation Forest.\n",
    "        If None, a default set of columns will be used.\n",
    "    contamination : float, optional\n",
    "        The proportion of outliers in the data set. Used by IsolationForest for thresholding.\n",
    "        Default is 0.01 (1%).\n",
    "    random_state : int, optional\n",
    "        Random seed for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A copy of proc_df with two extra columns:\n",
    "        - 'Anomaly_Label': +1 (normal) or -1 (anomalous)\n",
    "        - 'Anomaly_Score': continuous score (lower = more anomalous)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1) Default feature columns if not provided\n",
    "    feature_cols = [\n",
    "            'Stock_Daily_Returns',\n",
    "            'Volume_zscore',\n",
    "            'Volatility',]\n",
    "    \n",
    "    # Create a copy so we don't modify the original DataFrame\n",
    "    df_model = proc_df.copy()\n",
    "    \n",
    "    # 2) Drop rows with NaN in the feature columns\n",
    "    df_model.dropna(subset=feature_cols, inplace=True)\n",
    "    \n",
    "    # 3) Create the feature matrix X\n",
    "    X = df_model[feature_cols].values\n",
    "    \n",
    "    # 4) Build and train the Isolation Forest\n",
    "    iso_forest = IsolationForest(\n",
    "        n_estimators=100,\n",
    "        contamination=contamination,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    iso_forest.fit(X)\n",
    "    \n",
    "    # 5) Predict anomaly labels and get anomaly scores\n",
    "    anomaly_labels = iso_forest.predict(X)         # +1 or -1\n",
    "    anomaly_scores = iso_forest.decision_function(X)  # higher = more normal, lower = more anomalous\n",
    "    \n",
    "    # 6) Add these to df_model\n",
    "    df_model['Anomaly_Label'] = anomaly_labels\n",
    "    df_model['Anomaly_Score'] = anomaly_scores\n",
    "    \n",
    "    # 7) Merge back to the original proc_df by index\n",
    "    #    so we align rows that had valid feature data\n",
    "    df_result = proc_df.copy()\n",
    "    df_result = df_result.merge(\n",
    "        df_model[['Anomaly_Label', 'Anomaly_Score']],\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8fa5da64-6f60-409e-9177-03c56a05883d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Stock_Price</th>\n",
       "      <th>Stock_Daily_Returns</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Volume_change</th>\n",
       "      <th>Volume_pct_change</th>\n",
       "      <th>Volume_zscore</th>\n",
       "      <th>Volatility</th>\n",
       "      <th>Expected_Return</th>\n",
       "      <th>Expected_Price</th>\n",
       "      <th>Market_Price</th>\n",
       "      <th>Anomaly_Label</th>\n",
       "      <th>Anomaly_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2021-03-02</td>\n",
       "      <td>612.856750</td>\n",
       "      <td>0.012489</td>\n",
       "      <td>3051386</td>\n",
       "      <td>521143.0</td>\n",
       "      <td>0.205966</td>\n",
       "      <td>-0.739066</td>\n",
       "      <td>0.016092</td>\n",
       "      <td>-0.000684</td>\n",
       "      <td>610.591399</td>\n",
       "      <td>8767.250000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.178690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2021-03-03</td>\n",
       "      <td>617.266602</td>\n",
       "      <td>0.007196</td>\n",
       "      <td>3601961</td>\n",
       "      <td>550575.0</td>\n",
       "      <td>0.180434</td>\n",
       "      <td>-0.383187</td>\n",
       "      <td>0.016139</td>\n",
       "      <td>-0.003696</td>\n",
       "      <td>618.866593</td>\n",
       "      <td>8683.849609</td>\n",
       "      <td>1</td>\n",
       "      <td>0.212821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2021-03-04</td>\n",
       "      <td>611.645264</td>\n",
       "      <td>-0.009107</td>\n",
       "      <td>2621353</td>\n",
       "      <td>-980608.0</td>\n",
       "      <td>-0.272243</td>\n",
       "      <td>-0.704443</td>\n",
       "      <td>0.015819</td>\n",
       "      <td>0.002592</td>\n",
       "      <td>612.312652</td>\n",
       "      <td>8844.599609</td>\n",
       "      <td>1</td>\n",
       "      <td>0.187307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>603.455750</td>\n",
       "      <td>-0.013389</td>\n",
       "      <td>2839628</td>\n",
       "      <td>218275.0</td>\n",
       "      <td>0.083268</td>\n",
       "      <td>-0.603164</td>\n",
       "      <td>0.016018</td>\n",
       "      <td>0.001091</td>\n",
       "      <td>603.958905</td>\n",
       "      <td>8956.750000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.185361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2021-03-08</td>\n",
       "      <td>597.059265</td>\n",
       "      <td>-0.010600</td>\n",
       "      <td>1939925</td>\n",
       "      <td>-899703.0</td>\n",
       "      <td>-0.316838</td>\n",
       "      <td>-0.900343</td>\n",
       "      <td>0.016415</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>597.577680</td>\n",
       "      <td>8996.250000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.168854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>2025-02-07</td>\n",
       "      <td>1021.200012</td>\n",
       "      <td>-0.002101</td>\n",
       "      <td>941811</td>\n",
       "      <td>-230668.0</td>\n",
       "      <td>-0.196735</td>\n",
       "      <td>-0.889246</td>\n",
       "      <td>0.026734</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>1028.425260</td>\n",
       "      <td>10831.400391</td>\n",
       "      <td>1</td>\n",
       "      <td>0.075645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>2025-02-10</td>\n",
       "      <td>1027.500000</td>\n",
       "      <td>0.006169</td>\n",
       "      <td>1418998</td>\n",
       "      <td>477187.0</td>\n",
       "      <td>0.506670</td>\n",
       "      <td>-0.474225</td>\n",
       "      <td>0.026488</td>\n",
       "      <td>0.007075</td>\n",
       "      <td>1035.073340</td>\n",
       "      <td>10746.049805</td>\n",
       "      <td>1</td>\n",
       "      <td>0.094160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>2025-02-11</td>\n",
       "      <td>1013.799988</td>\n",
       "      <td>-0.013333</td>\n",
       "      <td>1204012</td>\n",
       "      <td>-214986.0</td>\n",
       "      <td>-0.151505</td>\n",
       "      <td>-0.619072</td>\n",
       "      <td>0.027176</td>\n",
       "      <td>0.007371</td>\n",
       "      <td>1018.416021</td>\n",
       "      <td>10724.400391</td>\n",
       "      <td>1</td>\n",
       "      <td>0.078863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>2025-02-12</td>\n",
       "      <td>1029.750000</td>\n",
       "      <td>0.015733</td>\n",
       "      <td>1082392</td>\n",
       "      <td>-121620.0</td>\n",
       "      <td>-0.101012</td>\n",
       "      <td>-0.650881</td>\n",
       "      <td>0.027369</td>\n",
       "      <td>0.004553</td>\n",
       "      <td>1036.590289</td>\n",
       "      <td>10640.950195</td>\n",
       "      <td>1</td>\n",
       "      <td>0.065206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>2025-02-13</td>\n",
       "      <td>1022.799988</td>\n",
       "      <td>-0.006749</td>\n",
       "      <td>1169309</td>\n",
       "      <td>86917.0</td>\n",
       "      <td>0.080301</td>\n",
       "      <td>-0.539471</td>\n",
       "      <td>0.020540</td>\n",
       "      <td>0.006643</td>\n",
       "      <td>1013.810019</td>\n",
       "      <td>10604.349609</td>\n",
       "      <td>1</td>\n",
       "      <td>0.177373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>979 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date  Stock_Price  Stock_Daily_Returns   Volume  Volume_change  \\\n",
       "11  2021-03-02   612.856750             0.012489  3051386       521143.0   \n",
       "12  2021-03-03   617.266602             0.007196  3601961       550575.0   \n",
       "13  2021-03-04   611.645264            -0.009107  2621353      -980608.0   \n",
       "14  2021-03-05   603.455750            -0.013389  2839628       218275.0   \n",
       "15  2021-03-08   597.059265            -0.010600  1939925      -899703.0   \n",
       "..         ...          ...                  ...      ...            ...   \n",
       "985 2025-02-07  1021.200012            -0.002101   941811      -230668.0   \n",
       "986 2025-02-10  1027.500000             0.006169  1418998       477187.0   \n",
       "987 2025-02-11  1013.799988            -0.013333  1204012      -214986.0   \n",
       "988 2025-02-12  1029.750000             0.015733  1082392      -121620.0   \n",
       "989 2025-02-13  1022.799988            -0.006749  1169309        86917.0   \n",
       "\n",
       "     Volume_pct_change  Volume_zscore  Volatility  Expected_Return  \\\n",
       "11            0.205966      -0.739066    0.016092        -0.000684   \n",
       "12            0.180434      -0.383187    0.016139        -0.003696   \n",
       "13           -0.272243      -0.704443    0.015819         0.002592   \n",
       "14            0.083268      -0.603164    0.016018         0.001091   \n",
       "15           -0.316838      -0.900343    0.016415         0.000834   \n",
       "..                 ...            ...         ...              ...   \n",
       "985          -0.196735      -0.889246    0.026734         0.003908   \n",
       "986           0.506670      -0.474225    0.026488         0.007075   \n",
       "987          -0.151505      -0.619072    0.027176         0.007371   \n",
       "988          -0.101012      -0.650881    0.027369         0.004553   \n",
       "989           0.080301      -0.539471    0.020540         0.006643   \n",
       "\n",
       "     Expected_Price  Market_Price  Anomaly_Label  Anomaly_Score  \n",
       "11       610.591399   8767.250000              1       0.178690  \n",
       "12       618.866593   8683.849609              1       0.212821  \n",
       "13       612.312652   8844.599609              1       0.187307  \n",
       "14       603.958905   8956.750000              1       0.185361  \n",
       "15       597.577680   8996.250000              1       0.168854  \n",
       "..              ...           ...            ...            ...  \n",
       "985     1028.425260  10831.400391              1       0.075645  \n",
       "986     1035.073340  10746.049805              1       0.094160  \n",
       "987     1018.416021  10724.400391              1       0.078863  \n",
       "988     1036.590289  10640.950195              1       0.065206  \n",
       "989     1013.810019  10604.349609              1       0.177373  \n",
       "\n",
       "[979 rows x 13 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_insider_trading(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c3f7463b-ca17-4de2-a5c6-60a3183d718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_insider_trading(processed_data).to_excel(\"results_data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b081ee5d-9f08-4bfe-af1a-1ebb5bf4aaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_insider_trading_dbscan(proc_df, eps=0.9, min_samples=5):\n",
    "    \"\"\"\n",
    "    Applies DBSCAN clustering to detect potential insider trading anomalies in a preprocessed DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    proc_df : pd.DataFrame\n",
    "        Preprocessed DataFrame (e.g., from your data_processor function) that contains columns like\n",
    "        'Stock_Daily_Returns', 'Volume_zscore', 'Volatility', etc.\n",
    "    feature_cols : list of str, optional\n",
    "        The list of feature column names to use. If None, defaults to:\n",
    "            ['Stock_Daily_Returns', 'Volume_zscore', 'Volatility']\n",
    "    eps : float, optional\n",
    "        The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n",
    "        (Default is 0.5)\n",
    "    min_samples : int, optional\n",
    "        The number of samples (or total weight) in a neighborhood for a point to be considered as a core point.\n",
    "        (Default is 5)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A copy of proc_df with two extra columns:\n",
    "            - 'DBSCAN_Label': Cluster label from DBSCAN (-1 indicates an anomaly).\n",
    "            - 'DBSCAN_Score': Currently set to NaN (DBSCAN doesn't provide a continuous score by default).\n",
    "    \"\"\"\n",
    "    # 1. Set default feature columns if not provided.\n",
    "    feature_cols = ['Stock_Daily_Returns', 'Volume_zscore', 'Volatility']\n",
    "    \n",
    "    # 2. Create a working copy and drop rows with missing values in these features.\n",
    "    df_model = proc_df.copy()\n",
    "    df_model.dropna(subset=feature_cols, inplace=True)\n",
    "    \n",
    "    # 3. Extract feature matrix\n",
    "    X = df_model[feature_cols].values\n",
    "    \n",
    "    # 4. Scale features (DBSCAN is sensitive to scale)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # 5. Initialize and fit DBSCAN\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    dbscan_labels = dbscan.fit_predict(X_scaled)\n",
    "    \n",
    "    # 6. Save the DBSCAN results.\n",
    "    # DBSCAN assigns -1 to points considered as noise (i.e., anomalies)\n",
    "    df_model['DBSCAN_Label'] = dbscan_labels\n",
    "    # Optionally, you could compute a custom score (e.g., distance to cluster centroid)\n",
    "    # For now, we set 'DBSCAN_Score' to NaN.\n",
    "    df_model['DBSCAN_Score'] = np.nan\n",
    "    \n",
    "    # 7. Merge the DBSCAN results back into the original proc_df.\n",
    "    df_result = proc_df.copy()\n",
    "    # We merge on index so that rows with valid feature data get the DBSCAN info;\n",
    "    # rows that were dropped remain NaN in these new columns.\n",
    "    df_result = df_result.merge(\n",
    "        df_model[['DBSCAN_Label', 'DBSCAN_Score']],\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f7809dfa-ad64-4560-84f1-205c22080d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Date  Stock_Price  Stock_Daily_Returns  Volume_zscore  Volatility  \\\n",
      "29  2021-03-30   609.513184             0.000955       2.805355    0.018134   \n",
      "42  2021-04-20   659.134766             0.024093       1.963469    0.022786   \n",
      "51  2021-05-04   625.359131            -0.047039       1.636778    0.020009   \n",
      "54  2021-05-07   609.610046            -0.036901       2.588555    0.020630   \n",
      "192 2021-11-26   747.318665            -0.050879       2.189166    0.021572   \n",
      "194 2021-11-30   758.917786             0.016449       2.287808    0.021699   \n",
      "240 2022-02-03   719.441589            -0.005859       2.261694    0.020301   \n",
      "255 2022-02-24   660.568542            -0.062980       1.457985    0.024184   \n",
      "256 2022-02-25   691.564575             0.046923      -0.432937    0.028794   \n",
      "259 2022-03-03   669.682129            -0.033685      -0.012055    0.031037   \n",
      "260 2022-03-04   653.014404            -0.024889       1.537067    0.029959   \n",
      "262 2022-03-08   666.416748             0.036145       1.245415    0.033108   \n",
      "263 2022-03-09   681.476257             0.022598      -0.653423    0.034130   \n",
      "264 2022-03-10   705.893005             0.035829       0.542372    0.035977   \n",
      "267 2022-03-15   729.676147             0.036483       2.800901    0.026771   \n",
      "300 2022-05-05   756.724731            -0.033549       2.709737    0.024186   \n",
      "310 2022-05-19   703.066223            -0.043749      -0.324381    0.017763   \n",
      "318 2022-05-31   740.544312             0.012797       2.680864    0.024038   \n",
      "393 2022-09-16   781.264221            -0.050394       1.916067    0.021539   \n",
      "786 2024-04-24  1102.548584            -0.053863       2.696407    0.022472   \n",
      "787 2024-04-25  1098.675293            -0.003513       1.791386    0.022306   \n",
      "908 2024-10-21  1017.049988            -0.069700       2.828757    0.023502   \n",
      "941 2024-12-09   933.950012            -0.041562       2.386755    0.017381   \n",
      "980 2025-02-01  1069.849976             0.044113       0.769364    0.024534   \n",
      "\n",
      "     DBSCAN_Label  \n",
      "29             -1  \n",
      "42             -1  \n",
      "51             -1  \n",
      "54             -1  \n",
      "192            -1  \n",
      "194            -1  \n",
      "240            -1  \n",
      "255            -1  \n",
      "256            -1  \n",
      "259            -1  \n",
      "260            -1  \n",
      "262            -1  \n",
      "263            -1  \n",
      "264            -1  \n",
      "267            -1  \n",
      "300            -1  \n",
      "310            -1  \n",
      "318            -1  \n",
      "393            -1  \n",
      "786            -1  \n",
      "787            -1  \n",
      "908            -1  \n",
      "941            -1  \n",
      "980            -1  \n"
     ]
    }
   ],
   "source": [
    "result_df = detect_insider_trading_dbscan(processed_data)\n",
    "\n",
    "# To see which days are flagged as anomalies:\n",
    "anomalies = result_df[result_df['DBSCAN_Label'] == -1]\n",
    "DB_scan_results = anomalies[['Date', 'Stock_Price', 'Stock_Daily_Returns', 'Volume_zscore', 'Volatility', 'DBSCAN_Label']]\n",
    "DB_scan_results.to_excel((\"DBSCAN_results_data.xlsx\"))\n",
    "\n",
    "print(anomalies[['Date', 'Stock_Price', 'Stock_Daily_Returns', 'Volume_zscore', 'Volatility', 'DBSCAN_Label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb659f0-dffe-467f-a10b-7c42bb22fb88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
